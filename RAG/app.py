# --- app.py ---

import streamlit as st
import boto3
import os
import time
import asyncio
from dotenv import load_dotenv
from pathlib import Path

# Import c√°c pipeline ƒë√£ x√¢y d·ª±ng
from indexing_pipeline import IndexingPipeline
from retrieval_pipeline import RetrievalPipeline
from config import RAGConfig

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# --- C·∫•u h√¨nh v√† Kh·ªüi t·∫°o ---
st.set_page_config(page_title="Vietnamese RAG System", layout="wide")

# L·∫•y c·∫•u h√¨nh t·ª´ .env
S3_BUCKET_NAME = os.getenv("S3_BUCKET_NAME")
AWS_REGION = os.getenv("AWS_REGION")

# Kh·ªüi t·∫°o S3 client
s3_client = boto3.client(
    's3',
    aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
    region_name=AWS_REGION
)

# Kh·ªüi t·∫°o RAG config
rag_config = RAGConfig()
rag_config.vector_store_type = 'opensearch'
rag_config.reranker.enabled = True 

@st.cache_resource
def get_retrieval_pipeline():
    try:
        return RetrievalPipeline(config=rag_config)
    except Exception as e:
        st.error(f"L·ªói khi kh·ªüi t·∫°o Retrieval Pipeline: {e}. ƒê·∫£m b·∫£o OpenSearch/Bedrock ƒë√£ ho·∫°t ƒë·ªông v√† c√≥ quy·ªÅn truy c·∫≠p.")
        return None

retrieval_pipeline = get_retrieval_pipeline()

def upload_to_s3(file_obj, bucket, object_name):
    """Upload m·ªôt file object l√™n S3 bucket."""
    try:
        s3_client.upload_fileobj(file_obj, bucket, object_name)
        return True
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i file l√™n S3: {e}")
        return False

def simulate_markdown_conversion(uploaded_filename):
    """Gi·∫£ l·∫≠p qu√° tr√¨nh chuy·ªÉn ƒë·ªïi sang Markdown."""
    st.info(f"Gi·∫£ l·∫≠p chuy·ªÉn ƒë·ªïi '{uploaded_filename}' sang Markdown...")
    time.sleep(2) 
    temp_dir = Path("./temp_markdowns")
    temp_dir.mkdir(exist_ok=True)
    markdown_content = f"# Ti√™u ƒë·ªÅ t·ª´ file {uploaded_filename}\n\nƒê√¢y l√† n·ªôi dung ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi."
    markdown_path = temp_dir / f"{Path(uploaded_filename).stem}.md"
    with open(markdown_path, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    st.success("Chuy·ªÉn ƒë·ªïi sang Markdown ho√†n t·∫•t.")
    return markdown_path


async def run_indexing(markdown_path):
    """Ch·∫°y pipeline indexing v√† ƒëo th·ªùi gian."""
    indexing_pipeline = IndexingPipeline(config=rag_config, use_bedrock_tokenizer=True)
    with open(markdown_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    start_time = time.time()
    with st.spinner("‚è≥ ƒêang x·ª≠ l√Ω v√† t·∫°o ch·ªâ m·ª•c cho t√†i li·ªáu..."):
        result = await indexing_pipeline.process_and_index_document(content, str(markdown_path))
    end_time = time.time()
    processing_time = end_time - start_time
    
    if result['status'] == 'success':
        st.success(f"‚úÖ L·∫≠p ch·ªâ m·ª•c th√†nh c√¥ng!")
        st.metric(label="**Th·ªùi gian Indexing**", value=f"{processing_time:.2f} gi√¢y")
    else:
        st.error(f"‚ùå L·ªói khi l·∫≠p ch·ªâ m·ª•c: {result['error']}")
    os.remove(markdown_path)


# --- Giao di·ªán Streamlit ---

st.title("H·ªá th·ªëng RAG Ti·∫øng Vi·ªát (OpenSearch + Reranker + Bedrock)")
st.markdown("---")

# C·ªôt b√™n tr√°i: Upload v√† Indexing
with st.sidebar:
    st.header("üìö Qu·∫£n l√Ω t√†i li·ªáu")
    st.subheader("1. T·∫£i l√™n t√†i li·ªáu m·ªõi")
    
    uploaded_file = st.file_uploader(
        "Ch·ªçn file (PDF, DOCX, TXT, EXCEL, CSV, PNG...)", 
        type=['pdf', 'docx', 'txt', 'xlsx', 'csv', 'png', 'PNG']
    )

    if uploaded_file is not None:
        st.write(f"ƒê√£ ch·ªçn: `{uploaded_file.name}`")
        if st.button("B·∫Øt ƒë·∫ßu x·ª≠ l√Ω"):
            with st.status("**ƒêang x·ª≠ l√Ω file...**", expanded=True) as status:
                st.write("B∆∞·ªõc 1: T·∫£i file l√™n S3...")
                upload_success = upload_to_s3(uploaded_file, S3_BUCKET_NAME, f"uploads/{uploaded_file.name}")
                if not upload_success:
                    status.update(label="T·∫£i l√™n th·∫•t b·∫°i!", state="error", expanded=True)
                else:
                    st.write("   => T·∫£i l√™n S3 th√†nh c√¥ng.")
                    st.write("B∆∞·ªõc 2: Chuy·ªÉn ƒë·ªïi sang Markdown...")
                    markdown_file_path = simulate_markdown_conversion(uploaded_file.name)
                    st.write(f"   => File Markdown ƒë∆∞·ª£c t·∫°o t·∫°i: `{markdown_file_path}`")
                    st.write("B∆∞·ªõc 3: L·∫≠p ch·ªâ m·ª•c t√†i li·ªáu...")
                    asyncio.run(run_indexing(markdown_file_path))
                    status.update(label="Ho√†n t·∫•t x·ª≠ l√Ω!", state="complete", expanded=True)
                    st.balloons()

# C·ªôt ch√≠nh: Retrieval
st.header("üí¨ Truy v·∫•n th√¥ng tin")

if retrieval_pipeline is None:
    st.warning("Pipeline truy v·∫•n ch∆∞a s·∫µn s√†ng. Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u h√¨nh v√† k·∫øt n·ªëi.")
else:
    # S·ª≠ d·ª•ng session state ƒë·ªÉ l∆∞u tr·ªØ l·ªãch s·ª≠ chat
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Hi·ªÉn th·ªã c√°c tin nh·∫Øn ƒë√£ c√≥
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Nh·∫≠n input t·ª´ ng∆∞·ªùi d√πng
    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y..."):
        # Th√™m c√¢u h·ªèi v√†o l·ªãch s·ª≠ chat
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # X·ª≠ l√Ω v√† hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
        with st.chat_message("assistant"):
            response_container = st.empty()
            with st.spinner("ƒêang t√¨m ki·∫øm v√† t·∫°o c√¢u tr·∫£ l·ªùi t·ª´ Bedrock..."):
                # G·ªçi pipeline retrieval ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi cu·ªëi c√πng
                response = retrieval_pipeline.search(query=prompt, k=5, strategy='hierarchical')

                # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi t·ª´ LLM
                response_container.markdown(response['answer'])

                # Hi·ªÉn th·ªã c√°c ngu·ªìn ƒë√£ s·ª≠ d·ª•ng trong expander
                if response['sources']:
                    with st.expander("Xem c√°c ngu·ªìn tham kh·∫£o"):
                        for i, source in enumerate(response['sources']):
                            st.write(f"**Ngu·ªìn {i+1} (Score: {source['final_score']:.4f})**")
                            st.info(source['content'])
        
        # Th√™m c√¢u tr·∫£ l·ªùi v√†o l·ªãch s·ª≠ chat
        st.session_state.messages.append({"role": "assistant", "content": response['answer']})