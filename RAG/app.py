# --- app.py ---

import streamlit as st
import boto3
import os
import time
import asyncio
from dotenv import load_dotenv
from pathlib import Path

try:
    from unstructured.partition.auto import partition
    from unstructured.staging.base import convert_to_markdown
except ImportError:
    st.error("Th∆∞ vi·ªán 'unstructured' ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Vui l√≤ng ch·∫°y: pip install \"unstructured[docx,xlsx,csv]\"")
    partition = None

try:
    from llama_parse import LlamaParse
except ImportError:
    st.error("Th∆∞ vi·ªán 'llama-parse' ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Vui l√≤ng ch·∫°y: pip install llama-parse")
    LlamaParse = None

# Import c√°c pipeline
from indexing_pipeline import IndexingPipeline
from retrieval_pipeline import RetrievalPipeline
from config import RAGConfig

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# --- C·∫•u h√¨nh v√† Kh·ªüi t·∫°o ---
st.set_page_config(page_title="Vietnamese RAG System", layout="wide")

S3_BUCKET_NAME = os.getenv("S3_BUCKET_NAME")
AWS_REGION = os.getenv("AWS_REGION")
LLAMA_API_KEY = os.getenv("LLAMA_CLOUD_API_KEY")

s3_client = boto3.client('s3', aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"), aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"), region_name=AWS_REGION)
rag_config = RAGConfig()
rag_config.vector_store_type = 'opensearch'
rag_config.reranker.enabled = True 

@st.cache_resource
def get_retrieval_pipeline():
    try:
        return RetrievalPipeline(config=rag_config)
    except Exception as e:
        st.error(f"L·ªói khi kh·ªüi t·∫°o Retrieval Pipeline: {e}")
        return None

retrieval_pipeline = get_retrieval_pipeline()

# --- Parsers & S3 ---

def upload_to_s3(file_path_or_obj, bucket, object_name):
    """T·∫£i file l√™n S3."""
    try:
        if isinstance(file_path_or_obj, str) or isinstance(file_path_or_obj, Path):
            s3_client.upload_file(str(file_path_or_obj), bucket, object_name)
        else:
            s3_client.upload_fileobj(file_path_or_obj, bucket, object_name)
        return True
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i file l√™n S3: {e}")
        return False

async def parse_with_llamaparse(file_path: str) -> str | None:
    """S·ª≠ d·ª•ng LlamaParse cho PDF v√† ·∫£nh (c√°c t√°c v·ª• c·∫ßn OCR)."""
    st.info("S·ª≠ d·ª•ng LlamaParse (cho PDF/·∫£nh)...")
    if not LlamaParse or not LLAMA_API_KEY:
        st.error("LlamaParse ho·∫∑c API key ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh.")
        return None
    parser = LlamaParse(
        api_key=LLAMA_API_KEY, 
        result_type="markdown", 
        num_workers=4,
        verbose=True, 
        language="vi",
        )
    try:
        documents = await parser.aload_data(file_path=file_path)
        return documents[0].text if documents else None
    except Exception as e:
        st.error(f"L·ªói v·ªõi LlamaParse: {e}")
        return None

def parse_with_unstructured(file_path: str) -> str | None:
    """S·ª≠ d·ª•ng th∆∞ vi·ªán Unstructured cho c√°c file office (DOCX, XLSX, CSV)."""
    st.info("S·ª≠ d·ª•ng Unstructured.io ƒë·ªÉ tr√≠ch xu·∫•t...")
    if not partition:
        st.error("Th∆∞ vi·ªán Unstructured ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o.")
        return None
    try:
        # H√†m partition t·ª± ƒë·ªông nh·∫≠n di·ªán v√† x·ª≠ l√Ω file
        elements = partition(filename=file_path)
        # Chuy·ªÉn ƒë·ªïi c√°c elements (Title, Table, NarrativeText, etc.) th√†nh m·ªôt chu·ªói markdown duy nh·∫•t
        return convert_to_markdown(elements)
    except Exception as e:
        st.error(f"L·ªói v·ªõi Unstructured: {e}")
        return None

# ==========================================================
# B√¥r sung h√†m x·ª≠ l√Ω reformat_markdown ƒë·ªÉ upload l√™n s3
# ==========================================================
async def reformat_markdown(file_path: str):
    """
    T·∫°o l·∫°i ƒë·ªãnh d·∫°ng ƒë·∫ßu m·ª•c heading cho vƒÉn b·∫£n
    """
    pass

async def dispatch_file_parser(file_path: str) -> Path | None:
    """
    ƒêi·ªÅu ph·ªëi file ƒë·∫øn ƒë√∫ng tr√¨nh ph√¢n t√≠ch d·ª±a tr√™n ƒëu√¥i file.
    Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n ƒë·∫øn file markdown ƒë√£ ƒë∆∞·ª£c t·∫°o.
    """
    file_extension = Path(file_path).suffix.lower()
    markdown_text = None

    # L·ª±a ch·ªçn parser ph√π h·ª£p
    if file_extension in ['.pdf', '.jpg', '.jpeg', '.png']:
        markdown_text = await parse_with_llamaparse(file_path)
    elif file_extension in ['.docx', '.xlsx', '.xls', '.csv']:
        # Unstructured x·ª≠ l√Ω t·ªët c√°c ƒë·ªãnh d·∫°ng n√†y
        markdown_text = parse_with_unstructured(file_path)
    else:
        st.warning(f"Kh√¥ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng file '{file_extension}'.")
        return None
    
    # L∆∞u k·∫øt qu·∫£ markdown v√†o file t·∫°m v√† tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n
    if markdown_text:
        temp_dir = Path("./temp_markdowns")
        temp_dir.mkdir(exist_ok=True)
        output_filename = f"{Path(file_path).stem}.md"
        markdown_path = temp_dir / output_filename
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write(markdown_text)
        st.success("Tr√≠ch xu·∫•t n·ªôi dung th√†nh c√¥ng!")
        return markdown_path
    else:
        st.error("Tr√≠ch xu·∫•t n·ªôi dung th·∫•t b·∫°i, kh√¥ng c√≥ vƒÉn b·∫£n ƒë∆∞·ª£c tr·∫£ v·ªÅ.")
        return None

async def run_indexing(markdown_path: Path):
    """Ch·∫°y pipeline indexing v√† ƒëo th·ªùi gian."""
    indexing_pipeline = IndexingPipeline(config=rag_config, use_bedrock_tokenizer=True)
    with open(markdown_path, 'r', encoding='utf-8') as f:
        content = f.read()
    start_time = time.time()
    with st.spinner("‚è≥ ƒêang x·ª≠ l√Ω v√† t·∫°o ch·ªâ m·ª•c cho t√†i li·ªáu..."):
        result = await indexing_pipeline.process_and_index_document(content, str(markdown_path))
    end_time = time.time()
    processing_time = end_time - start_time
    if result['status'] == 'success':
        st.success(f"‚úÖ L·∫≠p ch·ªâ m·ª•c th√†nh c√¥ng!")
        st.metric(label="**Th·ªùi gian Indexing**", value=f"{processing_time:.2f} gi√¢y")
    else:
        st.error(f"‚ùå L·ªói khi l·∫≠p ch·ªâ m·ª•c: {result['error']}")

# --- Giao di·ªán Streamlit ---
st.title("H·ªá th·ªëng RAG Ti·∫øng Vi·ªát (Unstructured + LlamaParse)")
st.markdown("---")

with st.sidebar:
    st.header("üìö Qu·∫£n l√Ω t√†i li·ªáu")
    st.subheader("1. T·∫£i l√™n t√†i li·ªáu m·ªõi")
    
    uploaded_file = st.file_uploader(
        "Ch·ªçn file (PDF, DOCX, XLSX, CSV, PNG, JPG...)", 
        type=['pdf', 'docx', 'xlsx', 'xls', 'csv', 'png', 'jpg', 'jpeg']
    )

    if uploaded_file is not None:
        st.write(f"ƒê√£ ch·ªçn: `{uploaded_file.name}`")
        if st.button("B·∫Øt ƒë·∫ßu x·ª≠ l√Ω"):
            with st.status("**ƒêang x·ª≠ l√Ω file...**", expanded=True) as status:
                
                # B∆∞·ªõc 1: Upload file g·ªëc l√™n S3
                s3_original_path = f"uploads/{uploaded_file.name}"
                st.write(f"B∆∞·ªõc 1: T·∫£i file g·ªëc l√™n S3 t·∫°i: `{s3_original_path}`...")
                upload_success = upload_to_s3(uploaded_file, S3_BUCKET_NAME, s3_original_path)
                if not upload_success:
                    status.update(label="T·∫£i file g·ªëc l√™n th·∫•t b·∫°i!", state="error", expanded=True)
                    st.stop()
                st.write("   => T·∫£i l√™n S3 th√†nh c√¥ng.")
                
                # B∆∞·ªõc 2: L∆∞u file t·∫°m ƒë·ªÉ parser ƒë·ªçc
                temp_upload_dir = Path("./temp_uploads")
                temp_upload_dir.mkdir(exist_ok=True)
                temp_file_path = temp_upload_dir / uploaded_file.name
                with open(temp_file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())

                # B∆∞·ªõc 3: ƒêi·ªÅu ph·ªëi file v√† t·∫°o markdown
                st.write("B∆∞·ªõc 2: Tr√≠ch xu·∫•t n·ªôi dung sang Markdown...")
                local_markdown_path = asyncio.run(dispatch_file_parser(str(temp_file_path)))
                
                # X√≥a file upload t·∫°m
                os.remove(temp_file_path)

                # N·∫øu c√≥ markdown, upload l√™n S3 v√† ti·∫øn h√†nh indexing
                if local_markdown_path:
                    st.write(f"   => File Markdown ƒë∆∞·ª£c t·∫°o t·∫°i: `{local_markdown_path}`")
                    
                    # B∆∞·ªõc 4: Upload file markdown ƒë√£ x·ª≠ l√Ω l√™n S3
                    s3_markdown_path = f"processed-markdowns/{local_markdown_path.name}"
                    st.write(f"B∆∞·ªõc 3: T·∫£i file markdown l√™n S3 t·∫°i: `{s3_markdown_path}`...")
                    upload_md_success = upload_to_s3(local_markdown_path, S3_BUCKET_NAME, s3_markdown_path)
                    if upload_md_success:
                        st.write("   => T·∫£i l√™n S3 th√†nh c√¥ng.")
                    
                    # B∆∞·ªõc 5: Ch·∫°y pipeline indexing
                    st.write("B∆∞·ªõc 4: L·∫≠p ch·ªâ m·ª•c t√†i li·ªáu...")
                    asyncio.run(run_indexing(local_markdown_path))
                    
                    status.update(label="Ho√†n t·∫•t x·ª≠ l√Ω!", state="complete", expanded=True)
                    st.balloons()
                    
                    # X√≥a file markdown t·∫°m tr√™n m√°y local sau khi ƒë√£ xong vi·ªác
                    os.remove(local_markdown_path)
                else:
                    status.update(label="Tr√≠ch xu·∫•t n·ªôi dung th·∫•t b·∫°i!", state="error", expanded=True)


# --- C·ªôt ch√≠nh: Retrieval ---
st.header("üí¨ Truy v·∫•n th√¥ng tin")
if retrieval_pipeline is None:
    st.warning("Pipeline truy v·∫•n ch∆∞a s·∫µn s√†ng.")
else:
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            response_container = st.empty()
            with st.spinner("ƒêang t√¨m ki·∫øm v√† t·∫°o c√¢u tr·∫£ l·ªùi..."):
                response = retrieval_pipeline.search(query=prompt, k=5, strategy='hierarchical')
                response_container.markdown(response['answer'])
                if response['sources']:
                    with st.expander("Xem c√°c ngu·ªìn tham kh·∫£o"):
                        for i, source in enumerate(response['sources']):
                            st.write(f"**Ngu·ªìn {i+1} (Score: {source['final_score']:.4f})**")
                            st.info(source['content'])
        
        st.session_state.messages.append({"role": "assistant", "content": response['answer']})